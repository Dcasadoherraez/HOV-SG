{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8044afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import hydra\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import open3d as o3d\n",
    "import open_clip\n",
    "import plyfile\n",
    "from scipy.spatial import cKDTree\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import BallTree\n",
    "import torch\n",
    "import torchmetrics as tm\n",
    "from hydra import initialize, compose\n",
    "\n",
    "from hovsg.labels.label_constants import (\n",
    "    SCANNET_COLOR_MAP_20, \n",
    "    SCANNET_LABELS_20, \n",
    "    TRUCKSCENES_LABELS,\n",
    "    TRUCKSCENES_COLORMAP\n",
    ")\n",
    "\n",
    "from hovsg.utils.eval_utils import (\n",
    "    load_feature_map,\n",
    "    knn_interpolation,\n",
    "    read_ply_and_assign_colors,\n",
    "    read_semantic_classes,\n",
    "    sim_2_label,\n",
    "    read_semantic_classes_replica,\n",
    "    text_prompt,\n",
    "    read_ply_and_assign_colors_replica\n",
    ")\n",
    "from hovsg.utils.metric import (\n",
    "    frequency_weighted_iou,\n",
    "    mean_iou,\n",
    "    mean_accuracy,\n",
    "    pixel_accuracy,\n",
    "    per_class_iou,\n",
    ")\n",
    "\n",
    "\n",
    "IGNORE_CLASS_INDEX = 12\n",
    "TRUCKSCENES_LABELS_TO_IDX = {\n",
    "    \"animal\": 0,\n",
    "    \"human.pedestrian.adult\": 7,\n",
    "    \"human.pedestrian.child\": 7,\n",
    "    \"human.pedestrian.construction_worker\": 7,\n",
    "    \"human.pedestrian.personal_mobility\": 7,\n",
    "    \"human.pedestrian.police_officer\": 7,\n",
    "    \"human.pedestrian.stroller\": IGNORE_CLASS_INDEX,\n",
    "    \"human.pedestrian.wheelchair\": IGNORE_CLASS_INDEX,\n",
    "    \"movable_object.barrier\": 1,\n",
    "    \"movable_object.debris\": IGNORE_CLASS_INDEX,\n",
    "    \"movable_object.pushable_pullable\": IGNORE_CLASS_INDEX,\n",
    "    \"movable_object.trafficcone\": 8,\n",
    "    \"static_object.bicycle_rack\": IGNORE_CLASS_INDEX,\n",
    "    \"static_object.traffic_sign\": 9,\n",
    "    \"vehicle.bicycle\": 2,\n",
    "    \"vehicle.bus.bendy\": 3,\n",
    "    \"vehicle.bus.rigid\": 3,\n",
    "    \"vehicle.car\": 4,\n",
    "    \"vehicle.construction\": 6,\n",
    "    \"vehicle.emergency.ambulance\": IGNORE_CLASS_INDEX,\n",
    "    \"vehicle.emergency.police\": IGNORE_CLASS_INDEX,\n",
    "    \"vehicle.motorcycle\": 5,\n",
    "    \"vehicle.trailer\": 10,\n",
    "    \"vehicle.truck\": 11,\n",
    "    \"vehicle.train\": IGNORE_CLASS_INDEX,\n",
    "    \"vehicle.other\": IGNORE_CLASS_INDEX,\n",
    "    \"vehicle.ego_trailer\": IGNORE_CLASS_INDEX,\n",
    "    \"unlabeled\": IGNORE_CLASS_INDEX\n",
    "}\n",
    "\n",
    "TRUCKSCENES_LABELS = (\n",
    "    'animal', \n",
    "    'barrier', \n",
    "    'bicycle', \n",
    "    'bus', \n",
    "    'car', \n",
    "    'motorcycle', \n",
    "    'construction vehicle', \n",
    "    'person', \n",
    "    'traffic cone',\n",
    "    'traffic sign', \n",
    "    'trailer', \n",
    "    'truck', \n",
    "    'other'\n",
    ")\n",
    "\n",
    "# Manually initialize Hydra and load the config\n",
    "config_path = \"../../config\"\n",
    "config_name = \"eval_sem_seg\"\n",
    "\n",
    "# Hydra context for manual loading\n",
    "with initialize(version_base=None, config_path=config_path):\n",
    "    params = compose(config_name=config_name)\n",
    "\n",
    "\n",
    "\n",
    "scenes_path = \"/home/daniel/spatial_understanding/benchmarks/HOV-SG/data/splits/truckscenes_no_dark_no_highway_val.txt\"\n",
    "with open(scenes_path, 'r') as f:\n",
    "    scenes = sorted([line.strip() for line in f.readlines()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0087d399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scene-0044384af3d8494e913fb8b14915239e-11'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_name = scenes[0]\n",
    "scene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528c6238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-31): 32 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-23): 24 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 1024)\n",
       "  (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def process_scene(scene_name):\n",
    "#     global params\n",
    "\n",
    "# load CLIP model\n",
    "if params.models.clip.type == \"ViT-L/14@336px\":\n",
    "    clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-L-14\",\n",
    "        pretrained=str(params.models.clip.checkpoint),\n",
    "        device=params.main.device,\n",
    "    )\n",
    "    clip_feat_dim = 768\n",
    "elif params.models.clip.type == \"ViT-H-14\":\n",
    "    clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-H-14\",\n",
    "        pretrained=str(params.models.clip.checkpoint),\n",
    "        device=params.main.device,\n",
    "    )\n",
    "    clip_feat_dim = 1024\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba3557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_frame(frame_path):\n",
    "    with open(frame_path, 'rb') as f:\n",
    "        feature_frame = pickle.load(f)\n",
    "    return feature_frame[\"points\"], np.array(feature_frame[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_0.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.2956521739130435\n",
      "IoU for class trailer: 0.1601423487544484\n",
      "IoU for class truck: 0.0\n",
      "miou:  0.15193150755583062\n",
      "fmiou:  0.16553624718400728\n",
      "macc:  0.618252148550216\n",
      "pacc:  0.8086124401913876\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_1.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.0\n",
      "IoU for class trailer: 0.0\n",
      "miou:  nan\n",
      "fmiou:  nan\n",
      "macc:  nan\n",
      "pacc:  0\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_2.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.4322033898305085\n",
      "IoU for class trailer: 0.0\n",
      "IoU for class truck: 0.009615384615384616\n",
      "miou:  0.22090938722294656\n",
      "fmiou:  0.3411844348610971\n",
      "macc:  0.5357142857142857\n",
      "pacc:  0.8\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_3.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.36343115124153497\n",
      "IoU for class trailer: 0.22764227642276422\n",
      "IoU for class truck: 0.06327800829875518\n",
      "miou:  0.2181171453210181\n",
      "fmiou:  0.24282239781207132\n",
      "macc:  0.8547547192141547\n",
      "pacc:  0.8762278978388998\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_4.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.46443514644351463\n",
      "IoU for class trailer: 0.0845771144278607\n",
      "IoU for class truck: 0.00477326968973747\n",
      "miou:  0.18459517685370427\n",
      "fmiou:  0.1726380581020944\n",
      "macc:  0.4258365833694448\n",
      "pacc:  0.36681222707423583\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_5.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.0\n",
      "IoU for class trailer: 0.2\n",
      "miou:  0.2\n",
      "fmiou:  0.2\n",
      "macc:  1.0\n",
      "pacc:  1.0\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_6.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.4155844155844156\n",
      "IoU for class trailer: 0.0\n",
      "IoU for class truck: 0.6551724137931034\n",
      "miou:  0.5353784146887595\n",
      "fmiou:  0.522985932022793\n",
      "macc:  0.8653846153846154\n",
      "pacc:  0.8793103448275862\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_7.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.34411764705882353\n",
      "IoU for class trailer: 0.13450292397660818\n",
      "IoU for class truck: 0.027439024390243903\n",
      "miou:  0.16868653180855853\n",
      "fmiou:  0.21161543929821514\n",
      "macc:  0.804758276183219\n",
      "pacc:  0.7745454545454545\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_8.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.411214953271028\n",
      "IoU for class trailer: 0.06944444444444445\n",
      "IoU for class truck: 0.001088139281828074\n",
      "miou:  0.1605825123324335\n",
      "fmiou:  0.20886260601525522\n",
      "macc:  0.4337719298245614\n",
      "pacc:  0.5462555066079295\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_9.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class barrier: 0.0\n",
      "IoU for class bus: 0.0\n",
      "IoU for class car: 0.08333333333333333\n",
      "IoU for class traffic cone: 0.0\n",
      "IoU for class trailer: 0.30201342281879195\n",
      "miou:  0.19267337807606263\n",
      "fmiou:  0.27628635346756153\n",
      "macc:  1.0\n",
      "pacc:  1.0\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_10.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.4125\n",
      "IoU for class trailer: 0.001584786053882726\n",
      "IoU for class truck: 0.2900763358778626\n",
      "miou:  0.23472037397724843\n",
      "fmiou:  0.3187136592840195\n",
      "macc:  0.7972508591065292\n",
      "pacc:  0.549618320610687\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_11.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.23491027732463296\n",
      "IoU for class trailer: 0.05161290322580645\n",
      "IoU for class truck: 0.0\n",
      "miou:  0.09550772685014647\n",
      "fmiou:  0.16565276888383768\n",
      "macc:  0.42635658914728686\n",
      "pacc:  0.7272727272727273\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_12.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.25\n",
      "IoU for class trailer: 0.31167400881057267\n",
      "IoU for class truck: 0.0046801872074883\n",
      "miou:  0.18878473200602033\n",
      "fmiou:  0.25781705379474273\n",
      "macc:  0.5495156352759333\n",
      "pacc:  0.6332737030411449\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_13.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.17142857142857143\n",
      "IoU for class trailer: 0.35323383084577115\n",
      "miou:  0.2623312011371713\n",
      "fmiou:  0.33906718725482055\n",
      "macc:  1.0\n",
      "pacc:  1.0\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_14.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.4588235294117647\n",
      "IoU for class trailer: 0.001564945226917058\n",
      "IoU for class truck: 0.44954128440366975\n",
      "miou:  0.30330991968078386\n",
      "fmiou:  0.4487166979643907\n",
      "macc:  0.9724358974358974\n",
      "pacc:  0.956989247311828\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_15.pkl\n",
      "################ scene-0044384af3d8494e913fb8b14915239e-3 ################\n",
      "IoU for class car: 0.46445497630331756\n",
      "IoU for class trailer: 0.030104712041884817\n",
      "IoU for class truck: 0.00904977375565611\n",
      "miou:  0.16786982070028614\n",
      "fmiou:  0.2187190949062387\n",
      "macc:  0.46479647964796483\n",
      "pacc:  0.5605381165919282\n",
      "Evaluating scene scene-0044384af3d8494e913fb8b14915239e-3 on frame mask_and_feat_16.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load Feature Map\u001b[39;00m\n\u001b[1;32m     14\u001b[0m masked_pcd, mask_feats \u001b[38;5;241m=\u001b[39m load_feature_frame(labelled_frame_path)\n\u001b[0;32m---> 16\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mtext_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_feat_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRUCKSCENES_LABELS_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m labels \u001b[38;5;241m=\u001b[39m sim_2_label(sim, labels_id)\n\u001b[1;32m     18\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels)\n",
      "File \u001b[0;32m~/spatial_understanding/benchmarks/HOV-SG/hovsg/utils/eval_utils.py:91\u001b[0m, in \u001b[0;36mtext_prompt\u001b[0;34m(clip_model, clip_feat_dim, mask_feats, text, templates)\u001b[0m\n\u001b[1;32m     89\u001b[0m text_list \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m templates:\n\u001b[0;32m---> 91\u001b[0m     text_feats \u001b[38;5;241m=\u001b[39m \u001b[43mget_text_feats_multiple_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_feat_dim\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     text_feats \u001b[38;5;241m=\u001b[39m get_text_feats(text_list, clip_model, clip_feat_dim)\n",
      "File \u001b[0;32m~/spatial_understanding/benchmarks/HOV-SG/hovsg/utils/clip_utils.py:328\u001b[0m, in \u001b[0;36mget_text_feats_multiple_templates\u001b[0;34m(in_text, clip_model, clip_feat_dim, batch_size)\u001b[0m\n\u001b[1;32m    326\u001b[0m multi_temp_landmarks_other \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mformat(lm) \u001b[38;5;28;01mfor\u001b[39;00m lm \u001b[38;5;129;01min\u001b[39;00m in_text \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m mul_tmp]\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# format the text with multiple templates except for \"background\"\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m text_feats \u001b[38;5;241m=\u001b[39m \u001b[43mget_text_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_temp_landmarks_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_feat_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# average the features\u001b[39;00m\n\u001b[1;32m    330\u001b[0m text_feats \u001b[38;5;241m=\u001b[39m text_feats\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(mul_tmp), text_feats\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/spatial_understanding/benchmarks/HOV-SG/hovsg/utils/clip_utils.py:160\u001b[0m, in \u001b[0;36mget_text_feats\u001b[0;34m(in_text, clip_model, clip_feat_dim, batch_size)\u001b[0m\n\u001b[1;32m    158\u001b[0m text_batch \u001b[38;5;241m=\u001b[39m text_tokens[text_id : text_id \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 160\u001b[0m     batch_feats \u001b[38;5;241m=\u001b[39m \u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    161\u001b[0m batch_feats \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_feats\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    162\u001b[0m batch_feats \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(batch_feats\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/anaconda3/envs/hovsg/lib/python3.9/site-packages/open_clip/model.py:290\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text, normalize)\u001b[0m\n\u001b[1;32m    288\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask)\n\u001b[1;32m    289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)  \u001b[38;5;66;03m# [batch_size, n_ctx, transformer.width]\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtext_global_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_pool_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection, nn\u001b[38;5;241m.\u001b[39mLinear):\n",
      "File \u001b[0;32m~/anaconda3/envs/hovsg/lib/python3.9/site-packages/open_clip/transformer.py:851\u001b[0m, in \u001b[0;36mtext_global_pool\u001b[0;34m(x, text, pool_type)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pool_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;66;03m# take features from the eot embedding (eot_token is the highest number in each sequence)\u001b[39;00m\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 851\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    853\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scene_path = os.path.join(params.main.feature_map_path, scene_name, \"labelled_frames\" )\n",
    "labelled_frame_names = sorted(os.listdir(scene_path), key=lambda x: int(x.split('.')[0].split('_')[-1]))\n",
    "\n",
    "# read semantic classes\n",
    "scene_name = params.main.scene_name\n",
    "TRUCKSCENES_LABELS_list = list(TRUCKSCENES_LABELS)\n",
    "labels_id = list(TRUCKSCENES_COLORMAP.keys())\n",
    "\n",
    "for labelled_frame_name in labelled_frame_names:\n",
    "    labelled_frame_path = os.path.join(scene_path, labelled_frame_name)\n",
    "    print(f\"Evaluating scene {scene_name} on frame {labelled_frame_name}\")\n",
    "\n",
    "    # Load Feature Map\n",
    "    masked_pcd, mask_feats = load_feature_frame(labelled_frame_path)\n",
    "\n",
    "    sim = text_prompt(clip_model, clip_feat_dim, mask_feats, TRUCKSCENES_LABELS_list, templates=True)\n",
    "    labels = sim_2_label(sim, labels_id)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # create a new pcd from the labeld pcd masks\n",
    "    colors = np.array([TRUCKSCENES_COLORMAP[i] for i in labels]) / 255.0\n",
    "    colors_map = TRUCKSCENES_COLORMAP\n",
    "    colors_map = {int(k): np.array(v) / 255.0 for k, v in colors_map.items()}\n",
    "\n",
    "    # load ground truth pcd\n",
    "    scene_map_path = f\"/shared/data/truckScenes/truckscenes_converted/trainval/{scene_name}/labelled_map.pth\"\n",
    "    xyz, feats, label, inst_label = torch.load(scene_map_path, weights_only=False)\n",
    "    label = np.array([TRUCKSCENES_LABELS_TO_IDX.get(l, IGNORE_CLASS_INDEX) for l in label], dtype=np.int64)\n",
    "\n",
    "    # Get corresponding labels for the masks using kdtree\n",
    "    full_pcd = xyz\n",
    "    full_pcd_kdtree = cKDTree(full_pcd[:, :3])\n",
    "    \n",
    "    # Create a mask for the labels that are not within the masked pcd\n",
    "    mask = np.zeros(len(full_pcd), dtype=bool)\n",
    "    \n",
    "    masked_pcd_modified = []\n",
    "    gt_labels = []\n",
    "    gt_points = []\n",
    "    \n",
    "    for pcd_mask in masked_pcd:\n",
    "        # Query neighbors for all masked_pcd points at once\n",
    "        dists, idxs = full_pcd_kdtree.query(pcd_mask, k=1)\n",
    "        \n",
    "        # Create a new point cloud with the masked points\n",
    "        pcd_mask_o3d = o3d.geometry.PointCloud()\n",
    "        pcd_mask_o3d.points = o3d.utility.Vector3dVector(xyz[idxs, :3])\n",
    "        masked_pcd_modified.append(pcd_mask_o3d)\n",
    "        \n",
    "        gt_labels.append(label[idxs])\n",
    "        gt_points.append(xyz[idxs, :3])\n",
    "        \n",
    "    gt_labels = np.concatenate(gt_labels, axis=0)\n",
    "    gt_points = np.concatenate(gt_points, axis=0)\n",
    "\n",
    "\n",
    "    gt_pcd = o3d.geometry.PointCloud()\n",
    "    gt_pcd.points = o3d.utility.Vector3dVector(gt_points)\n",
    "    \n",
    "    # assing colors to gt pcd based on labels\n",
    "    colors = np.zeros((len(gt_labels), 3))\n",
    "    for i, label in enumerate(gt_labels):\n",
    "        colors[i] = colors_map[label]\n",
    "    gt_pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    \n",
    "    ## FOR MASK BASED SEGMENTATION ##\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    for i in range(len(masked_pcd)):\n",
    "        pcd += masked_pcd_modified[i].paint_uniform_color(colors[i])\n",
    "\n",
    "    pred_labels = []\n",
    "    for i in range(len(masked_pcd_modified)):\n",
    "        pred_labels.append(np.repeat(labels[i], len(masked_pcd_modified[i].points)))\n",
    "    \n",
    "    pred_labels = np.hstack(pred_labels)\n",
    "    pred_labels = pred_labels.reshape(-1, 1)\n",
    "\n",
    "    gt_labels = gt_labels.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # concat coords and labels for predicied pcd\n",
    "    # coords_labels = np.zeros((len(pcd.points), 4))\n",
    "    # coords_labels[:, :3] = np.asarray(pcd.points)\n",
    "    # coords_labels[:, -1] = pred_labels[:, 0]\n",
    "    # # concat coords and labels for gt pcd\n",
    "    # coords_gt = np.zeros((len(gt_pcd.points), 4))\n",
    "    # coords_gt[:, :3] = np.asarray(gt_pcd.points)\n",
    "    # coords_gt[:, -1] = gt_labels[:, 0]\n",
    "    # # knn interpolation\n",
    "    # match_pc = knn_interpolation(coords_labels, coords_gt, k=5)\n",
    "    # pred_labels = match_pc[:, -1].reshape(-1, 1)\n",
    "    # ## MATCHING ##\n",
    "    # labels_gt = gt_labels\n",
    "    # label_pred = pred_labels\n",
    "    # assert len(labels_gt) == len(pred_labels)\n",
    "    \n",
    "    labels_gt, label_pred = gt_labels, pred_labels\n",
    "\n",
    "    # for gt, pred in zip(labels_gt, label_pred):\n",
    "    #     print(gt, pred)\n",
    "\n",
    "    # print(\"labels_gt\", labels_gt, \"pred_labels\", pred_labels)\n",
    "    ignore = [IGNORE_CLASS_INDEX]\n",
    "\n",
    "    # print(label_pred, labels_gt, ignore)\n",
    "    print(\"################ {} ################\".format(scene_name))\n",
    "    ious = per_class_iou(label_pred, labels_gt, ignore=ignore, classes=TRUCKSCENES_LABELS)\n",
    "    # print(\"per class iou: \", ious)\n",
    "    miou = mean_iou(label_pred, labels_gt, ignore=ignore)\n",
    "    print(\"miou: \", miou)\n",
    "    fmiou = frequency_weighted_iou(label_pred, labels_gt, ignore=ignore)\n",
    "    print(\"fmiou: \", fmiou)\n",
    "    macc = mean_accuracy(label_pred, labels_gt, ignore=ignore)\n",
    "    print(\"macc: \", macc)\n",
    "    pacc = pixel_accuracy(label_pred, labels_gt, ignore=ignore)\n",
    "    print(\"pacc: \", pacc)\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c1830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hovsg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
